{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electric-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-ladder",
   "metadata": {},
   "source": [
    "# Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assured-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from raw_data\n",
    "X_train = pd.read_csv('../../raw_data/clean_sentiment_data/X_train.csv')\n",
    "y_train = pd.read_csv('../../raw_data/clean_sentiment_data/y_train.csv')\n",
    "# X_test = pd.read_csv('../../raw_data/clean_sentiment_data/X_test.csv')\n",
    "# y_test = pd.read_csv('../../raw_data/clean_sentiment_data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knowing-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming y in pd.Series\n",
    "y_train = y_train['Answer.sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-cherry",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-eight",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alike-inspiration",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train_bin = y_train.apply(lambda x: 0 if x <= 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convenient-humor",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But (uhh) so yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The movie ruins it for itself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(umm) The acting's subpar, the screen the scr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And now, because of that, Machinima got intimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought something would have to descend on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10837</th>\n",
       "      <td>When we're talking about a verb, a verb can ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10838</th>\n",
       "      <td>If you are continuing at OSU, I hope you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10839</th>\n",
       "      <td>I come in and I say \"Okay, hi everybody I'm go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10840</th>\n",
       "      <td>This time, in the birthplace of modern innovat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10841</th>\n",
       "      <td>I think Steve Martin's done other better things</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10842 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Phrase\n",
       "0                                      But (uhh) so yeah\n",
       "1                          The movie ruins it for itself\n",
       "2       (umm) The acting's subpar, the screen the scr...\n",
       "3      And now, because of that, Machinima got intimi...\n",
       "4      I thought something would have to descend on m...\n",
       "...                                                  ...\n",
       "10837  When we're talking about a verb, a verb can ha...\n",
       "10838  If you are continuing at OSU, I hope you have ...\n",
       "10839  I come in and I say \"Okay, hi everybody I'm go...\n",
       "10840  This time, in the birthplace of modern innovat...\n",
       "10841    I think Steve Martin's done other better things\n",
       "\n",
       "[10842 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.drop(columns=['Segment_ID', 'clip', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-tablet",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basic preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-clearing",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sharp-stevens",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lower(x):\n",
    "    return x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "great-boost",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train['clean_text'] = X_train['Phrase'].apply(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-specific",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proud-senate",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def punct(x):\n",
    "    for text in string.punctuation:\n",
    "        x = x.replace(text, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "operational-perception",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train['clean_text'] = X_train['clean_text'].apply(punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-pharmacy",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stretch-haven",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def del_num(x):\n",
    "    return ''.join([letter for letter in x if not letter.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrong-nebraska",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train['clean_text'] = X_train['clean_text'].apply(del_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-superintendent",
   "metadata": {},
   "source": [
    "## First model with bag of words CountVectorizer & MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cathedral-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set on X_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train_bin, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intense-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_t['clean_text'])\n",
    "X_t_vect1 = vectorizer.transform(X_t['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "theoretical-father",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(X_t_vect1, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "quiet-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer on X_val\n",
    "X_val_vect1 = vectorizer.transform(X_val['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cubic-victim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute accuracy on validation set\n",
    "round(nb_model.score(X_val_vect1, y_val),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-structure",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploration of predicts from model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "existing-likelihood",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = nb_model.predict(X_val_vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lucky-inquiry",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "whole-saver",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results['y_val'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "developed-celebration",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results['y_pred'] = y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accepting-waterproof",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = results.drop(columns=['clip', 'ID', 'Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "unknown-olive",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results['results'] = 1 - abs(results['y_val'] - results['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "composed-jason",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2169, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "angry-gateway",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tete_faux = results[results['results'] == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "soviet-peoples",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tete_faux.to_csv('False_nlp_ml_m1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-animal",
   "metadata": {},
   "source": [
    "## Tf-Id vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "chronic-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a pipeline a grid search to know which parameters are better \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB ())\n",
    "])\n",
    "\n",
    "grid_params = {'vect__max_df': [0.7, 0.8, 0.9],\n",
    "              'vect__max_features': [30, 40, 50, 60],\n",
    "              'vect__ngram_range': [(1,1), (2,2)],\n",
    "              'vect__min_df': [0.8, 0.9, 1]}\n",
    "\n",
    "search = GridSearchCV(pipe, grid_params, n_jobs=-1, verbose=1, scoring='accuracy',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "suspected-donna",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.float64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__smooth_idf': True,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__use_idf': True,\n",
       " 'vect__vocabulary': None,\n",
       " 'model__alpha': 1.0,\n",
       " 'model__class_prior': None,\n",
       " 'model__fit_prior': True}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "lesser-exhaust",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   49.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acce...\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('model',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'vect__max_df': [0.7, 0.8, 0.9],\n",
       "                         'vect__max_features': [30, 40, 50, 60],\n",
       "                         'vect__min_df': [0.8, 0.9, 1],\n",
       "                         'vect__ngram_range': [(1, 1), (2, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_t['clean_text'], y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "elegant-universe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': 0.7,\n",
       " 'vect__max_features': 50,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "proved-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_good = Pipeline([\n",
    "    ('vect', TfidfVectorizer(max_df = 0.7, max_features = 50, ngram_range = (1,1))),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "japanese-wellington",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.7, max_features=50,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('model',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_good.fit(X_t['clean_text'], y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "undefined-museum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pipe_good.score(X_val['clean_text'], y_val),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-channels",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
